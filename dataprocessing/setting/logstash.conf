# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

input {
        file{
                path => "/home/sunmi/Downloads/exampleFile.csv"
                start_position => "beginning"
                sincedb_path => "/dev/null"
        }
	# mysql에서 정해진 시간마다 가져오는 경우
        # jdc {
        #        jdbc_validate_connection => true
        #        jdbc_driver_library => "/usr/share/logstash/tools/mysql-connector-java-5.1.38.jar"
        #        jdbc_driver_class => "com.mysql.jdbc.Driver"
        #        jdbc_connection_string => "jdbc:mysql://test-database-1.rds.amazonaws.com:3306/<DBname>"
        #        jdbc_user => "username"
        #        jdbc_password => "password"
        #        use_column_value => true
        #        tracking_column => idx
        #        last_run_metadata_path => "/usr/share/logstash/inspector-index.dat"
        #        statement => "select 문"
        #        schedule => "0 */2 * * *" 
        # }


}
filter{
        csv{
                separator => ","
                columns => ["shopInfo","datetime","IoTInfo","IoTType","IoTStat"]
        }
        
	date{
                match => ["datetime","yyyy-MM-dd HH"]
                timezone => "Asia/Seoul"
		locale => "ko"
                target => "@timestamp"
        }

	mutate{
		convert => {
			"IoTType" => "integer"
			"IoTStat" => "integer"
		}
	}
}
output {
	elasticsearch {
        	hosts => ["localhost:9200"]
                index => "log-%{+YYYY-MM-dd}"
                document_type => "_doc"
	}
  stdout { }
}


